model_name: MySmolLM2-135M

model_config:
  name: SmolLM2-135M
  hf_config:
    org: HuggingFaceTB
    name: SmolLM2-135M
  block_size: 8192
  vocab_size: 49152
  padded_vocab_size: 49152
  n_layer: 30
  n_head: 9  # 注意力头的数量
  head_size: 64 # 注意力头的维度
  n_embd: 576 # 输入的维度
  n_query_groups: 3 # 查询组的数量
  rotary_percentage: 1.0
  parallel_residual: False
  bias: False
  norm_class_name: RMSNorm
  mlp_class_name: LLaMAMLP
  intermediate_size: 1536
  rope_base: 100000
  norm_eps: 1e-5

out_dir: out/pretrain/smollm2-135m_tinystories

precision: bf16-mixed

initial_checkpoint_dir:

resume: false

data: TinyStories

train:
  save_interval: 5000
  log_interval: 1
  global_batch_size: 32
  micro_batch_size: 32
  lr_warmup_steps: 1000
  epochs: 
  max_tokens: 9700000000
  max_steps:
  max_seq_length: 256
  tie_embeddings: true
  max_norm: 1.0
  min_lr: 0.0

eval:
  interval: 2000
  max_new_tokens:
  max_iters: 100
  initial_validation: false
  final_validation: false

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0005
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.95

devices: auto

num_nodes: 1

tokenizer_dir: SmolLM2-135M

logger_name: wandb

seed: 42